{% set name = "transformers" %}
{% set version = "4.29.2" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz
  sha256: ed9467661f459f1ce49461d83f18f3b36b6a37f306182dc2ba272935f3b93ebb

build:
  skip: True  # [py<37]
  # s390x is missing pyrarrow that datasets depends on
  skip: True  # [linux and s390x]
  # At present, there are no 3.11 compatible Pytorch version for win or ppc
  # There is a bug requiring PyTorch 1.10 on osx-64 which is not available for 3.11 
  skip: True  # [py>310 and (ppc64le or win or (osx and x86_64))]
  number: 0
  script: {{ PYTHON }} -m pip install . --no-deps --no-build-isolation -vv
  entry_points:
    - transformers-cli=transformers.commands.transformers_cli:main

requirements:
  host:
    - pip
    - python
    - setuptools
    - wheel
  run:
    - python
    # datasets required for transformers-cli.
    - datasets
    - importlib-metadata  # [py<38]
    - filelock
    - huggingface_hub >=0.14.1,<1.0
    - numpy
    - packaging
    # TODO: Working around osx bug importing two openmp variants.
    - pytorch  # [not (osx and x86_64)]
    - pytorch <1.12,>1.9  # [osx and x86_64]
    - pyyaml
    - regex !=2019.12.17
    - requests
    - sacremoses
    - tokenizers >=0.11.1,!=0.11.3
    - tqdm >=4.27

test:
  requires:
    - pip
  imports:
    - transformers
    - transformers.benchmark
    - transformers.commands
    - transformers.data
    - transformers.data.datasets
    - transformers.data.metrics
    - transformers.data.processors
    - transformers.models
    - transformers.pipelines
    - transformers.sagemaker
    - transformers.utils
  commands:
    - pip check
    - transformers-cli --help

about:
  home: https://github.com/huggingface/transformers
  license: Apache-2.0
  license_family: Apache
  license_file: LICENSE
  summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow
  description: |
    Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.

    These models can be applied on:

    - üìù Text, for tasks like text classification, information extraction, question answering, summarization, translation, text generation, in over 100 languages.
    - üñºÔ∏è Images, for tasks like image classification, object detection, and segmentation.
    - üó£Ô∏è Audio, for tasks like speech recognition and audio classification.
    
    Transformer models can also perform tasks on several modalities combined, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.
  doc_url: https://huggingface.co/docs/transformers/index
  dev_url: https://github.com/huggingface/transformers

extra:
  recipe-maintainers:
    - mxr-conda
    - roccqqck
    - oblute
    - rluria14
    - ndmaxar
    - setu4993
    - hadim
    - wietsedv
  skip-lints:
    - python_build_tool_in_run
